{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SKxK3j0_AWQPnekXJdBRxsOEU2gP37al",
      "authorship_tag": "ABX9TyPdGa77aXpDvIZ9DNOoW1TZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alwnraj/compvisionalgo/blob/main/efficientnetalgo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M04YL2QKgem3",
        "outputId": "6432e6cd-ea5e-4561-97a9-096b118b6daa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 0, Loss: 1.6555, Position: [1.3405 0.6266 1.6575]\n",
            "Checkpoint time: 0.7590 seconds\n",
            "Frame 10, Loss: 0.0068, Position: [1.2582 0.6251 1.5661]\n",
            "Checkpoint time: 0.2506 seconds\n",
            "Frame 20, Loss: 0.0025, Position: [1.1772 0.6252 1.4713]\n",
            "Checkpoint time: 0.2556 seconds\n",
            "Frame 30, Loss: 0.0220, Position: [1.1165 0.6209 1.3836]\n",
            "Checkpoint time: 0.2517 seconds\n",
            "Frame 40, Loss: 0.0419, Position: [1.1096 0.6227 1.3785]\n",
            "Checkpoint time: 0.2496 seconds\n",
            "Frame 50, Loss: 0.0007, Position: [1.1695 0.6239 1.4841]\n",
            "Checkpoint time: 0.2752 seconds\n",
            "Frame 60, Loss: 0.0020, Position: [1.2535 0.6284 1.6032]\n",
            "Checkpoint time: 0.3792 seconds\n",
            "Frame 70, Loss: 0.0318, Position: [1.3282 0.6228 1.6883]\n",
            "Checkpoint time: 0.2642 seconds\n",
            "Frame 80, Loss: 0.0015, Position: [1.3963 0.6302 1.7508]\n",
            "Checkpoint time: 0.2604 seconds\n",
            "Frame 90, Loss: 0.0006, Position: [1.3518 0.6325 1.7038]\n",
            "Checkpoint time: 0.2579 seconds\n",
            "Frame 100, Loss: 0.0026, Position: [1.2514 0.6158 1.6012]\n",
            "Checkpoint time: 0.2672 seconds\n",
            "Frame 110, Loss: 0.0066, Position: [1.1555 0.627  1.4969]\n",
            "Checkpoint time: 0.2781 seconds\n",
            "Frame 120, Loss: 0.0023, Position: [1.0886 0.6352 1.3903]\n",
            "Checkpoint time: 0.4063 seconds\n",
            "Frame 130, Loss: 0.0055, Position: [1.0802 0.6303 1.3681]\n",
            "Checkpoint time: 0.3870 seconds\n",
            "Frame 140, Loss: 0.0013, Position: [1.1585 0.6217 1.4759]\n",
            "Checkpoint time: 0.2812 seconds\n",
            "Frame 150, Loss: 0.0033, Position: [1.2611 0.6134 1.6061]\n",
            "Checkpoint time: 0.2612 seconds\n",
            "Frame 160, Loss: 0.0027, Position: [1.3185 0.6322 1.6816]\n",
            "Checkpoint time: 0.2803 seconds\n",
            "Frame 170, Loss: 0.0032, Position: [1.3154 0.6434 1.6608]\n",
            "Checkpoint time: 0.4208 seconds\n",
            "Frame 180, Loss: 0.0005, Position: [1.2994 0.7241 1.6423]\n",
            "Checkpoint time: 0.2570 seconds\n",
            "Frame 190, Loss: 0.0056, Position: [1.2938 0.8512 1.6146]\n",
            "Checkpoint time: 0.2727 seconds\n",
            "Frame 200, Loss: 0.0031, Position: [1.3004 0.9571 1.6041]\n",
            "Checkpoint time: 0.2705 seconds\n",
            "Frame 210, Loss: 0.0008, Position: [1.3104 0.9063 1.6158]\n",
            "Checkpoint time: 0.2578 seconds\n",
            "Frame 220, Loss: 0.0018, Position: [1.3112 0.7632 1.6156]\n",
            "Checkpoint time: 0.3896 seconds\n",
            "Frame 230, Loss: 0.0010, Position: [1.3103 0.6309 1.6106]\n",
            "Checkpoint time: 0.2681 seconds\n",
            "Frame 240, Loss: 0.0049, Position: [1.2888 0.4964 1.5865]\n",
            "Checkpoint time: 0.2663 seconds\n",
            "Frame 250, Loss: 0.0020, Position: [1.2862 0.3784 1.5782]\n",
            "Checkpoint time: 0.2655 seconds\n",
            "Frame 260, Loss: 0.0033, Position: [1.2864 0.3074 1.5716]\n",
            "Checkpoint time: 0.2645 seconds\n",
            "Frame 270, Loss: 0.0010, Position: [1.2881 0.3669 1.568 ]\n",
            "Checkpoint time: 0.3937 seconds\n",
            "Frame 280, Loss: 0.0109, Position: [1.2875 0.4951 1.5742]\n",
            "Checkpoint time: 0.3908 seconds\n",
            "Frame 290, Loss: 0.0002, Position: [1.2885 0.6454 1.5805]\n",
            "Checkpoint time: 0.2754 seconds\n",
            "Frame 300, Loss: 0.0012, Position: [1.2913 0.7815 1.5839]\n",
            "Checkpoint time: 0.2630 seconds\n",
            "Frame 310, Loss: 0.0027, Position: [1.2931 0.9141 1.5983]\n",
            "Checkpoint time: 0.2614 seconds\n",
            "Frame 320, Loss: 0.0004, Position: [1.2985 0.9597 1.5858]\n",
            "Checkpoint time: 0.3673 seconds\n",
            "Frame 330, Loss: 0.0006, Position: [1.2985 0.8719 1.5918]\n",
            "Checkpoint time: 0.2680 seconds\n",
            "Frame 340, Loss: 0.0011, Position: [1.2801 0.7052 1.603 ]\n",
            "Checkpoint time: 0.2605 seconds\n",
            "Frame 350, Loss: 0.0012, Position: [1.2707 0.5565 1.5983]\n",
            "Checkpoint time: 0.2661 seconds\n",
            "Frame 360, Loss: 0.0084, Position: [1.2599 0.4174 1.586 ]\n",
            "Checkpoint time: 0.2570 seconds\n",
            "Frame 370, Loss: 0.0026, Position: [1.244  0.3005 1.573 ]\n",
            "Checkpoint time: 0.4127 seconds\n",
            "Frame 380, Loss: 0.0003, Position: [1.2388 0.2702 1.574 ]\n",
            "Checkpoint time: 0.2694 seconds\n",
            "Frame 390, Loss: 0.0023, Position: [1.2407 0.357  1.5532]\n",
            "Checkpoint time: 0.2539 seconds\n",
            "Frame 400, Loss: 0.0029, Position: [1.226  0.5305 1.5337]\n",
            "Checkpoint time: 0.2608 seconds\n",
            "Frame 410, Loss: 0.0000, Position: [1.2382 0.7067 1.5395]\n",
            "Checkpoint time: 0.2750 seconds\n",
            "Frame 420, Loss: 0.0003, Position: [1.243  0.842  1.5178]\n",
            "Checkpoint time: 0.4041 seconds\n",
            "Frame 430, Loss: 0.0035, Position: [1.2562 0.9025 1.5217]\n",
            "Checkpoint time: 0.2487 seconds\n",
            "Frame 440, Loss: 0.0012, Position: [1.2564 0.7678 1.5346]\n",
            "Checkpoint time: 0.2733 seconds\n",
            "Frame 450, Loss: 0.0008, Position: [1.2659 0.6088 1.5228]\n",
            "Checkpoint time: 0.2627 seconds\n",
            "Frame 460, Loss: 0.0020, Position: [1.2494 0.5822 1.5391]\n",
            "Checkpoint time: 0.2671 seconds\n",
            "Frame 470, Loss: 0.0008, Position: [1.1729 0.5843 1.5863]\n",
            "Checkpoint time: 0.3904 seconds\n",
            "Frame 480, Loss: 0.0001, Position: [1.0718 0.5857 1.6316]\n",
            "Checkpoint time: 0.3000 seconds\n",
            "Frame 490, Loss: 0.0052, Position: [1.0065 0.5929 1.6617]\n",
            "Checkpoint time: 0.2626 seconds\n",
            "Frame 500, Loss: 0.0029, Position: [1.0705 0.5944 1.6198]\n",
            "Checkpoint time: 0.2598 seconds\n",
            "Frame 510, Loss: 0.0004, Position: [1.195  0.5805 1.5711]\n",
            "Checkpoint time: 0.2540 seconds\n",
            "Frame 520, Loss: 0.0011, Position: [1.2985 0.5698 1.5187]\n",
            "Checkpoint time: 0.2533 seconds\n",
            "Frame 530, Loss: 0.0005, Position: [1.3828 0.5575 1.4475]\n",
            "Checkpoint time: 0.3571 seconds\n",
            "Frame 540, Loss: 0.0022, Position: [1.4215 0.5495 1.4192]\n",
            "Checkpoint time: 0.2561 seconds\n",
            "Frame 550, Loss: 0.0009, Position: [1.3409 0.5625 1.4704]\n",
            "Checkpoint time: 0.2626 seconds\n",
            "Frame 560, Loss: 0.0001, Position: [1.2456 0.5755 1.5194]\n",
            "Checkpoint time: 0.2742 seconds\n",
            "Frame 570, Loss: 0.0005, Position: [1.1634 0.5785 1.578 ]\n",
            "Checkpoint time: 0.2694 seconds\n",
            "Frame 580, Loss: 0.0009, Position: [1.0784 0.5821 1.6366]\n",
            "Checkpoint time: 0.3969 seconds\n",
            "Frame 590, Loss: 0.0005, Position: [1.0113 0.5862 1.6751]\n",
            "Checkpoint time: 0.2625 seconds\n",
            "Frame 600, Loss: 0.0004, Position: [1.0544 0.5842 1.6453]\n",
            "Checkpoint time: 0.2658 seconds\n",
            "Frame 610, Loss: 0.0001, Position: [1.1689 0.5695 1.584 ]\n",
            "Checkpoint time: 0.2543 seconds\n",
            "Frame 620, Loss: 0.0005, Position: [1.2734 0.5572 1.5254]\n",
            "Checkpoint time: 0.2604 seconds\n",
            "Frame 630, Loss: 0.0011, Position: [1.3496 0.5505 1.4706]\n",
            "Checkpoint time: 0.3920 seconds\n",
            "Frame 640, Loss: 0.0014, Position: [1.4206 0.5544 1.3996]\n",
            "Checkpoint time: 0.2700 seconds\n",
            "Frame 650, Loss: 0.0085, Position: [1.463  0.5611 1.3644]\n",
            "Checkpoint time: 0.2771 seconds\n",
            "Frame 660, Loss: 0.0002, Position: [1.4148 0.5639 1.3968]\n",
            "Checkpoint time: 0.2699 seconds\n",
            "Frame 670, Loss: 0.0004, Position: [1.3343 0.5704 1.4625]\n",
            "Checkpoint time: 0.2555 seconds\n",
            "Frame 680, Loss: 0.0002, Position: [1.2215 0.5787 1.5358]\n",
            "Checkpoint time: 0.2627 seconds\n",
            "Frame 690, Loss: 0.0005, Position: [1.1159 0.5852 1.5861]\n",
            "Checkpoint time: 0.4129 seconds\n",
            "Frame 700, Loss: 0.0003, Position: [1.0445 0.5812 1.6257]\n",
            "Checkpoint time: 0.2656 seconds\n",
            "Frame 710, Loss: 0.0003, Position: [1.0792 0.5819 1.6179]\n",
            "Checkpoint time: 0.2687 seconds\n",
            "Frame 720, Loss: 0.0022, Position: [1.1745 0.5827 1.5457]\n",
            "Checkpoint time: 0.2832 seconds\n",
            "Frame 730, Loss: 0.0003, Position: [1.2767 0.5773 1.4673]\n",
            "Checkpoint time: 0.2547 seconds\n",
            "Frame 740, Loss: 0.0035, Position: [1.3375 0.5834 1.4305]\n",
            "Checkpoint time: 0.3644 seconds\n",
            "Frame 750, Loss: 0.0003, Position: [1.3422 0.5862 1.4333]\n",
            "Checkpoint time: 0.2554 seconds\n",
            "Frame 760, Loss: 0.0025, Position: [1.3131 0.5841 1.4436]\n",
            "Checkpoint time: 0.5625 seconds\n",
            "Frame 770, Loss: 0.0007, Position: [1.3008 0.5823 1.4481]\n",
            "Checkpoint time: 0.2712 seconds\n",
            "Frame 780, Loss: 0.0001, Position: [1.2857 0.5846 1.447 ]\n",
            "Checkpoint time: 0.2579 seconds\n",
            "Frame 790, Loss: 0.0001, Position: [1.2802 0.582  1.4518]\n",
            "Checkpoint time: 0.2686 seconds\n",
            "SLAM completed. Total frames processed: 798\n",
            "Total runtime: 839.06 seconds\n",
            "Average feature extraction time: 0.0944 seconds\n",
            "Total features extracted: 798\n",
            "Total feature extraction time: 75.35 seconds\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "def read_file_list(filename):\n",
        "    \"\"\"\n",
        "    Reads a trajectory from a text file.\n",
        "    \"\"\"\n",
        "    file = open(filename)\n",
        "    data = file.read()\n",
        "    lines = data.replace(\",\",\" \").replace(\"\\t\",\" \").split(\"\\n\")\n",
        "    list = [[v.strip() for v in line.split(\" \") if v.strip()!=\"\"] for line in lines if len(line)>0 and line[0]!=\"#\"]\n",
        "    list = [(float(l[0]),l[1:]) for l in list if len(l)>1]\n",
        "    return dict(list)\n",
        "\n",
        "class TUM_RGBD_Dataset(Dataset):\n",
        "    def __init__(self, base_dir, transform=None):\n",
        "        self.base_dir = base_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.rgb_dict = read_file_list(os.path.join(base_dir, 'rgb.txt'))\n",
        "        self.depth_dict = read_file_list(os.path.join(base_dir, 'depth.txt'))\n",
        "        self.groundtruth = pd.read_csv(os.path.join(base_dir, 'groundtruth.txt'),\n",
        "                                       sep=' ', comment='#', header=None,\n",
        "                                       names=['timestamp', 'tx', 'ty', 'tz', 'qx', 'qy', 'qz', 'qw'])\n",
        "\n",
        "        # Synchronize RGB and depth images\n",
        "        self.rgb_timestamps = list(self.rgb_dict.keys())\n",
        "        self.depth_timestamps = list(self.depth_dict.keys())\n",
        "        self.synced_timestamps = self.synchronize_timestamps()\n",
        "\n",
        "    def synchronize_timestamps(self):\n",
        "        synced = []\n",
        "        for rgb_time in self.rgb_timestamps:\n",
        "            depth_time = min(self.depth_timestamps, key=lambda x: abs(x - rgb_time))\n",
        "            if abs(rgb_time - depth_time) < 0.02:  # 20ms threshold\n",
        "                synced.append((rgb_time, depth_time))\n",
        "        return synced\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.synced_timestamps)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rgb_time, depth_time = self.synced_timestamps[idx]\n",
        "\n",
        "        rgb_path = os.path.join(self.base_dir, self.rgb_dict[rgb_time][0])\n",
        "        depth_path = os.path.join(self.base_dir, self.depth_dict[depth_time][0])\n",
        "\n",
        "        rgb_img = Image.open(rgb_path).convert('RGB')\n",
        "        depth_img = Image.open(depth_path).convert('RGB')  # Convert depth to RGB\n",
        "\n",
        "        if self.transform:\n",
        "            rgb_img = self.transform(rgb_img)\n",
        "            depth_img = self.transform(depth_img)\n",
        "\n",
        "        # Get the closest ground truth pose\n",
        "        closest_gt = self.groundtruth.iloc[(self.groundtruth['timestamp'] - rgb_time).abs().argsort()[0]]\n",
        "        pose = closest_gt[['tx', 'ty', 'tz']].values\n",
        "\n",
        "        return rgb_img, depth_img, pose\n",
        "\n",
        "class EfficientNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EfficientNetFeatureExtractor, self).__init__()\n",
        "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
        "        self.efficientnet.classifier = nn.Identity()  # Remove the final classification layer\n",
        "\n",
        "        # Modify the first convolutional layer to accept 6 channels\n",
        "        self.efficientnet.features[0][0] = nn.Conv2d(6, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "\n",
        "        self.fc = nn.Linear(1280, 3)  # EfficientNet-B0 outputs 1280 features\n",
        "        self.feature_extraction_time = 0\n",
        "        self.feature_extraction_count = 0\n",
        "\n",
        "    def forward(self, rgb, depth):\n",
        "        start_time = time.time()\n",
        "        x = torch.cat((rgb, depth), dim=1)  # Combine RGB and depth channels\n",
        "        features = self.efficientnet(x)\n",
        "        pose = self.fc(features)\n",
        "        end_time = time.time()\n",
        "        self.feature_extraction_time += (end_time - start_time)\n",
        "        self.feature_extraction_count += 1\n",
        "        return pose, features\n",
        "\n",
        "class SLAM:\n",
        "    def __init__(self):\n",
        "        self.feature_extractor = EfficientNetFeatureExtractor().to(device)\n",
        "        self.optimizer = optim.Adam(self.feature_extractor.parameters(), lr=0.001)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.map = {}\n",
        "        self.current_position = np.array([0.0, 0.0, 0.0])\n",
        "        self.trajectory = [self.current_position]\n",
        "        self.checkpoint_times = []\n",
        "\n",
        "    def update(self, rgb, depth, gt_pose):\n",
        "        start_time = time.time()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        estimated_pose, features = self.feature_extractor(rgb, depth)\n",
        "\n",
        "        loss = self.criterion(estimated_pose, gt_pose.float())\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update position and map\n",
        "        self.current_position = gt_pose.detach().cpu().numpy().squeeze()\n",
        "        self.trajectory.append(self.current_position)\n",
        "        self.map[tuple(self.current_position)] = features.detach().cpu().numpy().squeeze()\n",
        "\n",
        "        end_time = time.time()\n",
        "        self.checkpoint_times.append((start_time, end_time))\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "def main():\n",
        "    base_dir = '/content/drive/MyDrive/Colab Notebooks/outputfolder/rgbd_dataset_freiburg1_xyz'\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # EfficientNet-B0 expects 224x224 input\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
        "    ])\n",
        "\n",
        "    dataset = TUM_RGBD_Dataset(base_dir, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    slam = SLAM()\n",
        "\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    for i, (rgb, depth, gt_pose) in enumerate(dataloader):\n",
        "        rgb, depth, gt_pose = rgb.to(device), depth.to(device), gt_pose.to(device)\n",
        "\n",
        "        loss = slam.update(rgb, depth, gt_pose)\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            checkpoint_start, checkpoint_end = slam.checkpoint_times[-1]\n",
        "            print(f\"Frame {i}, Loss: {loss:.4f}, Position: {slam.current_position}\")\n",
        "            print(f\"Checkpoint time: {checkpoint_end - checkpoint_start:.4f} seconds\")\n",
        "\n",
        "    total_end_time = time.time()\n",
        "\n",
        "    print(\"SLAM completed. Total frames processed:\", len(dataset))\n",
        "    print(f\"Total runtime: {total_end_time - total_start_time:.2f} seconds\")\n",
        "\n",
        "    avg_feature_extraction_time = slam.feature_extractor.feature_extraction_time / slam.feature_extractor.feature_extraction_count\n",
        "    print(f\"Average feature extraction time: {avg_feature_extraction_time:.4f} seconds\")\n",
        "    print(f\"Total features extracted: {slam.feature_extractor.feature_extraction_count}\")\n",
        "    print(f\"Total feature extraction time: {slam.feature_extractor.feature_extraction_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}